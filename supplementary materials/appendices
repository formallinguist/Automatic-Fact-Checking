##Computational Resources
All experiments are run on A100 GPUs. The runtime for each experiment varies as follows:
Simple Prompting takes two hours each, while Automatic Scraping
takes two and half hours. Naive RAG experiments require four hours for English and six
hours for Telugu to complete, whereas Advanced RAG takes six hours for English and eight
hours for Telugu to complete. 



##Hyperparameters used for Naive RAG and Advanced RAG pipeline

Chunking
chunk size: 500
chunk overlap: 50


Embedding
embedding dim: 1024

LLM Generation
temperature: 0.5
max tokens: 4000
top p: 1.0


## Fact-Checking Sources

| **Source**              | **URL** |
|--------------------------|---------|
| India Today              | [Link](https://www.indiatoday.in/fact-check) |
| Telugu Post Fact Check   | [Link](https://telugupost.com/fact-check) |
| News Meter               | [Link](https://newsmeter.in/top-stories) |
| News Mobile              | [Link](https://www.newsmobile.in/news/nation/) |
| PolitiFact               | [Link](https://www.politifact.com/) |
| DW                       | [Link](https://www.dw.com/en/top-stories/s-9097) |
| Fact Check               | [Link](https://www.factcheck.org/) |
| Times of India           | [Link](https://timesofindia.indiatimes.com/) |
| AFP Fact Check           | [Link](https://factcheck.afp.com/) |
| Hindustan Times          | [Link](https://www.hindustantimes.com/) |
| Press Information Bureau | [Link](https://www.pib.gov.in/factcheck.aspx) |
| India TV                 | [Link](https://www.indiatvnews.com/) |
| News 18                  | [Link](https://www.news18.com/) |
| Boom                     | [Link](https://www.boomlive.in/fact-check) |
| Geo News                 | [Link](https://www.geo.tv/category/geo-fact-check) |
| Digit Eye India          | [Link](https://digiteye.in/) |
| The Indian Express       | [Link](https://indianexpress.com/) |
| Republic World           | [Link](https://www.republicworld.com/) |
| Press Trust of India     | [Link](https://www.ptinews.com/fact-check) |
| Alt News                 | [Link](https://www.altnews.in/) |
| The Economic Times       | [Link](https://economictimes.indiatimes.com/) |
| NDTV                     | [Link](https://www.ndtv.com/topic/fact~check) |
| Factly                   | [Link](https://factly.in/) |
| Vishvasnews              | [Link](https://www.vishvasnews.com/english/home-english/) |
| The Quint                | [Link](https://www.thequint.com/) |
| USA Today                | [Link](https://eu.usatoday.com/) |
| DT Next                  | [Link](https://www.dtnext.in/) |
| The Week                 | [Link](https://www.theweek.in/) |
| Namibia Fact Check       | [Link](https://namibiafactcheck.org.na/) |
| Fact Crescendo           | [Link](https://english.factcrescendo.com/) |
| Logically Facts          | [Link](https://www.logicallyfacts.com/en) |
| DFRAC                    | [Link](https://dfrac.org/en/) |
| Pesa Check               | [Link](https://pesacheck.org/) |
| ABP                      | [Link](https://news.abplive.com/fact-check) |
| OP India                 | [Link](https://www.opindia.com/) |
| News Checker             | [Link](https://newschecker.in/) |
| Deccan Herald            | [Link](https://www.deccanherald.com/) |
| The Print                | [Link](https://theprint.in/) |
| DNA                      | [Link](https://www.dnaindia.com/) |
| Live Mint                | [Link](https://www.livemint.com/) |
| Gulf News                | [Link](https://gulfnews.com/uae) |
| Economic Times           | [Link](https://economictimes.indiatimes.com/?from=mdr) |
| Business World           | [Link](https://www.businessworld.in/) |
| Smithsonian Magazine     | [Link](https://www.smithsonianmag.com/) |
| The Hindu                | [Link](https://www.thehindu.com/) |
| BBC                      | [Link](https://www.bbc.com/) |
| The Statesman            | [Link](https://www.thestatesman.com/) |
| Wion News                | [Link](https://www.wionews.com/) |
| Zee News                 | [Link](https://zeenews.india.com/) |


# Inter Annotation Guidelines

## 1. Guidelines for True and False Labels

### Objective
Determine if the claim is factually accurate based on the supporting document. Annotators are instructed to assign one of two labels: **True** or **False**, following the below guidelines.

---

### Rules for Labels

#### **True**
- The claim is directly supported by the fact-checking article or news article and discussed in the gold justification.  
- Minor phrasing differences (e.g., *"helped identify clusters"* vs. *"detected clusters"*) are acceptable.  
- Claims with accurate numbers/dates (e.g., *"55,342 cases"*) are considered True even if the wording varies.  

**Example:**  
- **Claim:** "WHO praises Indiaâ€™s Aarogya Setu app for identifying clusters."  
- **Justification:** "WHO Director-General acknowledged the appâ€™s role in identifying clusters (Source X)."  
- **Label:** **True**

---

#### **False**
- The False claim is supported by fact-checking sources.  
- The supporting document provides sufficient evidence (from the fact-checking article).  
- The claim contains fabricated or incorrect information, as discussed in the fact-checking article and gold justifications.  
- **Partially True Claims:** Label as **False** if the claim is misleading despite containing partial truth, as outlined in the fact-checking article and gold justification.  

**Example:**  
- **Claim:** "COVID-19 cases dropped to 55,342 due to improved recovery rates."  
- **Gold Justification:** "The claim is False because the supporting document X says COVID-19 cases dropped to 45,000 and there is a high recovery rate."  
  *(Fact-checking article label: Partially True / Misleading / Partially False)*  
- **Label:** **False** (as it spreads misinformation)

---

## 2. Boolean Answers (Yes/No)

### Direct QA Pairs

**Objective:** Answer Yes/No questions based strictly on the claim and supporting document.

- **Yes:**  
  If the claim is **True**, and the question directly reflects the claim, and the supporting document agrees with it.  

- **No:**  
  If the claim is **False**, and the question directly reflects the claim, and the supporting document agrees with it.  

---

### In-Direct QA Pairs

**Objective:** Answer Yes/No questions based strictly on the claim and supporting document.

- **Yes:**  
  If the supporting document confirms a corrective or related question that helps show the claim is **False**.  

- **No:**  
  If the supporting document contradicts the question and supports the original claim is **True**.  







# ðŸš€ Claim Verification & RAG Prompts Collection
# Use these prompt templates for Zero-shot, Naive RAG, Advanced RAG, Scraping, QDEV,
# Question Generation, QA pairs, and Error Categorization.
# Includes English + Telugu variants.

# =========================
# ðŸ”¹ Zero-shot Prompts
# =========================

# English
prompt = f"Determine the validity of the following claim and label it as 'TRUE' or 'FAKE'. Provide only 'TRUE' or 'FAKE' on the first line, followed by a justification on the second line: {claim}"

# Telugu
prompt = f"Determine the validity of the following claim and label it as 'TRUE' or 'FAKE'. Provide only 'TRUE' or 'FAKE' on the first line, followed by a justification in Telugu in the second line: {claim}"


# =========================
# ðŸ”¹ Naive RAG
# =========================

# English: Retrieve supporting docs
cohere_prompt = f"Provide supporting documents for the claim: {claim}"

prompt = f"Here are the relevant documents: {all_docs_content} Based on the above information, provide in the first line only 'FAKE' or 'TRUE'. And provide the justification from the given context in the second line. '{claim}'."

# Telugu: Retrieve supporting docs
cohere_prompt = f"Provide supporting documents for the claim in Telugu: {claim}"

prompt = f"Here are the relevant documents: {all_docs_content} Based on the above information, provide in the first line only 'FAKE' or 'TRUE'. And provide the justification in Telugu from the given context in the second line. '{claim}'."


# =========================
# ðŸ”¹ Advanced RAG
# =========================

# Query rewriting
f"Rewrite the following query to improve retrieval performance: {query}"

# Query rewriting in Telugu
f"Rewrite the following query in Telugu to improve retrieval performance: {query}"

# Prompt compression
f"Compress the following prompt while retaining essential information: {prompt}"


# =========================
# ðŸ”¹ Automatic Scraping
# =========================

# Telugu
prompt = f"Label the following claim as 'True' or 'Fake' from the given context. Answer with either 'TRUE' or 'FAKE' only in the first line. Also add another line for justification in Telugu from the given context only. Context: {refined_context} . Claim: {claim}"

# English
prompt = f"Label the following claim as 'True' or 'Fake' from the given context. Answer with either 'TRUE' or 'FAKE' only in the first line. Also add another line for justification from the given context only. Context: {refined_context} . Claim: {claim}"


# =========================
# ðŸ”¹ QDEV (Question-driven Evidence Verification)
# =========================

# English
prompt = f"Label the following claim as 'True' or 'Fake' from the given context. Answer with either 'TRUE' or 'FAKE' only in the first line. Also add another line for justification from the given context only. Context: {contextQuestions} . Claim: {claim}"

# Telugu
prompt = f"Label the following claim as 'True' or 'Fake' from the given context. Answer with either 'TRUE' or 'FAKE' only in the first line. Also add another line for justification in Telugu from the given context only. Context: {contextQuestions} . Claim: {claim}"


# =========================
# ðŸ”¹ Question Generation
# =========================

# English
prompt = f"{questions} \n Claim: {claim}. Provide me only with similar questions relevant for the claim given in similar json format. Provide boolean, Abstractive, Extractive questions."

# Telugu
prompt = f"{questions} \n Claim: {claim}. Provide me only with similar questions relevant for the claim given in similar json format. Provide boolean, Abstractive, Extractive questions in Telugu."


# =========================
# ðŸ”¹ Question-Answer Pairs
# =========================

# English
prompt = f"{questions and answers} \n Claim: {claim}. Provide me only with similar questions and answer pairs relevant for the claim given in similar json format. Provide boolean, Abstractive, Extractive questions and answer pairs in a single list json. Do not leave question and answer pair empty."

# Telugu
prompt = f"{questions and answers} \n Claim: {claim}. Provide me only with similar questions and answer pairs in Telugu relevant for the claim given in similar json format. Provide boolean, Abstractive, Extractive questions and answer pairs in a single list json. Do not leave question and answer pair empty."


# =========================
# âš¡ Error Categorization
# =========================

# Telugu Quantified Errors
def get_rag_error_category(claim, predicted_label, actual_label, predicted_justification, actual_justification):
    prompt = (
        f"Classify the following errors into one of the categories: "
        f"Hallucinations, Biases, Retrieval errors and other.\n\n"
        f"Claim: {claim}\n"
        f"Predicted Label: {predicted_label}\n"
        f"Actual Label: {actual_label}\n"
        f"Predicted Justification: {predicted_justification}\n"
        f"Actual Justification: {actual_justification}\n\n"
        f"Provide only the error category on the first line, followed by a brief explanation."
    )
    return prompt

# English Quantified Errors
def get_rag_error_category(claim, predicted_label, actual_label, predicted_justification, actual_justification):
    prompt = (
        f"Classify the following errors into one of the categories: "
        f"Hallucinations, Biases, Translation errors, Retrieval errors and other.\n\n"
        f"Claim: {claim}\n"
        f"Predicted Label: {predicted_label}\n"
        f"Actual Label: {actual_label}\n"
        f"Predicted Justification: {predicted_justification}\n"
        f"Actual Justification: {actual_justification}\n\n"
        f"Provide only the error category on the first line, followed by a brief explanation."
    )
    return prompt

# Results of Models for three runs for English and  Telugu

| Model          | Approach | F1 (EN)      | METEOR (EN)  | R-L (EN)    | ChrF (EN)   | BLEURT (EN) | BERTScore (EN) | F1 (TE)      | METEOR (TE)  | R-L (TE)    | ChrF (TE)   | BLEURT (TE) | BERTScore (TE) |
|----------------|----------|--------------|--------------|-------------|-------------|-------------|----------------|--------------|--------------|-------------|-------------|-------------|----------------|
| **Llama-3-70B** | SP       | 80.16 Â± 4.28 | 0.288 Â± 0.001 | 0.283 Â± 0.001 | 39.92 Â± 0.55 | 0.48 Â± 0.01 | 0.87 Â± 0.01    | 42.95 Â± 3.25 | 0.126 Â± 0.001 | 0.165 Â± 0.001 | 25.34 Â± 0.44 | 0.45 Â± 0.01 | 0.72 Â± 0.01 |
|                | N-RAG    | 58.16 Â± 9.98 | 0.267 Â± 0.001 | 0.275 Â± 0.001 | 38.51 Â± 0.40 | 0.47 Â± 0.01 | 0.86 Â± 0.01    | 40.77 Â± 5.50 | 0.140 Â± 0.001 | 0.163 Â± 0.001 | 23.94 Â± 0.43 | 0.44 Â± 0.01 | 0.71 Â± 0.01 |
|                | A-RAG    | 61.21 Â± 0.87 | 0.256 Â± 0.001 | 0.259 Â± 0.001 | 41.11 Â± 0.44 | 0.56 Â± 0.01 | 0.88 Â± 0.01    | 44.31 Â± 4.50 | 0.134 Â± 0.001 | 0.174 Â± 0.001 | 26.08 Â± 0.22 | 0.48 Â± 0.01 | 0.72 Â± 0.01 |
|                | AS       | 86.14 Â± 5.89 | 0.281 Â± 0.001 | 0.289 Â± 0.001 | 37.72 Â± 0.30 | 0.47 Â± 0.01 | 0.89 Â± 0.01    | 80.45 Â± 9.50 | 0.123 Â± 0.001 | 0.162 Â± 0.001 | 24.70 Â± 0.55 | 0.45 Â± 0.01 | 0.72 Â± 0.01 |
| **Llama-3.3-70B** | SP    | 75.07 Â± 3.20 | 0.275 Â± 0.001 | 0.275 Â± 0.001 | 38.55 Â± 0.48 | 0.47 Â± 0.01 | 0.89 Â± 0.01    | 70.68 Â± 4.50 | 0.172 Â± 0.001 | 0.229 Â± 0.001 | 32.79 Â± 0.10 | 0.51 Â± 0.01 | 0.71 Â± 0.01 |
|                | N-RAG    | 57.44 Â± 8.70 | 0.286 Â± 0.001 | 0.282 Â± 0.001 | 35.41 Â± 0.79 | 0.43 Â± 0.01 | 0.87 Â± 0.01    | 38.36 Â± 3.58 | 0.106 Â± 0.001 | 0.123 Â± 0.001 | 27.04 Â± 0.23 | 0.40 Â± 0.01 | 0.72 Â± 0.01 |
|                | A-RAG    | 59.38 Â± 3.30 | 0.259 Â± 0.001 | 0.250 Â± 0.001 | 37.81 Â± 0.50 | 0.42 Â± 0.01 | 0.88 Â± 0.01    | 41.76 Â± 5.13 | 0.135 Â± 0.001 | 0.174 Â± 0.001 | 28.29 Â± 0.45 | 0.43 Â± 0.01 | 0.72 Â± 0.01 |
|                | AS       | 77.22 Â± 4.40 | 0.308 Â± 0.001 | 0.318 Â± 0.001 | 39.81 Â± 0.34 | 0.49 Â± 0.01 | 0.90 Â± 0.01    | 80.58 Â± 11.58 | 0.163 Â± 0.001 | 0.196 Â± 0.001 | 31.84 Â± 0.33 | 0.50 Â± 0.01 | 0.73 Â± 0.01 |
| **Llama-3-8B** | SP       | 56.21 Â± 4.40 | 0.294 Â± 0.001 | 0.279 Â± 0.001 | 39.85 Â± 0.32 | 0.50 Â± 0.01 | 0.89 Â± 0.01    | 48.45 Â± 3.50 | 0.138 Â± 0.001 | 0.194 Â± 0.001 | 29.64 Â± 0.18 | 0.42 Â± 0.01 | 0.73 Â± 0.01 |
|                | N-RAG    | 52.41 Â± 6.20 | 0.266 Â± 0.001 | 0.280 Â± 0.001 | 37.59 Â± 0.40 | 0.45 Â± 0.01 | 0.86 Â± 0.01    | 47.29 Â± 4.50 | 0.139 Â± 0.001 | 0.184 Â± 0.001 | 28.21 Â± 0.15 | 0.41 Â± 0.01 | 0.72 Â± 0.01 |
|                | A-RAG    | 60.11 Â± 3.24 | 0.254 Â± 0.001 | 0.304 Â± 0.001 | 38.41 Â± 0.66 | 0.49 Â± 0.01 | 0.89 Â± 0.01    | 49.75 Â± 6.50 | 0.133 Â± 0.001 | 0.203 Â± 0.001 | 29.61 Â± 0.30 | 0.44 Â± 0.01 | 0.72 Â± 0.01 |
|                | AS       | 70.83 Â± 4.40 | 0.288 Â± 0.001 | 0.291 Â± 0.001 | 38.64 Â± 0.49 | 0.47 Â± 0.01 | 0.87 Â± 0.01    | 50.77 Â± 5.50 | 0.124 Â± 0.001 | 0.164 Â± 0.001 | 25.96 Â± 0.37 | 0.42 Â± 0.01 | 0.72 Â± 0.01 |
| **Mixtral-8x7B** | SP     | 56.95 Â± 5.20 | 0.285 Â± 0.001 | 0.273 Â± 0.001 | 38.94 Â± 0.13 | 0.49 Â± 0.01 | 0.88 Â± 0.01    | 49.22 Â± 3.40 | 0.110 Â± 0.001 | 0.129 Â± 0.001 | 27.59 Â± 0.03 | 0.29 Â± 0.01 | 0.72 Â± 0.01 |
|                | N-RAG    | 57.19 Â± 3.20 | 0.293 Â± 0.001 | 0.303 Â± 0.001 | 37.51 Â± 0.15 | 0.47 Â± 0.01 | 0.86 Â± 0.01    | 51.24 Â± 5.60 | 0.153 Â± 0.001 | 0.172 Â± 0.001 | 28.39 Â± 0.02 | 0.41 Â± 0.01 | 0.73 Â± 0.01 |
|                | A-RAG    | 59.26 Â± 4.20 | 0.280 Â± 0.001 | 0.293 Â± 0.001 | 38.66 Â± 0.53 | 0.51 Â± 0.01 | 0.89 Â± 0.01    | 55.49 Â± 6.34 | 0.146 Â± 0.001 | 0.213 Â± 0.001 | 28.66 Â± 0.12 | 0.43 Â± 0.01 | 0.72 Â± 0.01 |
|                | AS       | 84.08 Â± 6.20 | 0.316 Â± 0.001 | 0.340 Â± 0.001 | 41.03 Â± 0.38 | 0.48 Â± 0.01 | 0.87 Â± 0.01    | 73.86 Â± 4.56 | 0.087 Â± 0.001 | 0.114 Â± 0.001 | 23.27 Â± 0.15 | 0.28 Â± 0.01 | 0.70 Â± 0.01 |
| **Gemma 2.9B** | SP       | 64.72 Â± 5.50 | 0.208 Â± 0.001 | 0.283 Â± 0.001 | 31.89 Â± 0.31 | 0.46 Â± 0.01 | 0.87 Â± 0.01    | 57.41 Â± 3.40 | 0.125 Â± 0.001 | 0.183 Â± 0.001 | 26.66 Â± 0.17 | 0.43 Â± 0.01 | 0.73 Â± 0.01 |
|                | N-RAG    | 62.21 Â± 3.20 | 0.197 Â± 0.001 | 0.264 Â± 0.001 | 30.51 Â± 0.50 | 0.45 Â± 0.01 | 0.87 Â± 0.01    | 52.39 Â± 5.20 | 0.103 Â± 0.001 | 0.173 Â± 0.001 | 28.41 Â± 0.16 | 0.43 Â± 0.01 | 0.72 Â± 0.01 |
|                | A-RAG    | 63.81 Â± 4.40 | 0.180 Â± 0.001 | 0.283 Â± 0.001 | 34.74 Â± 0.48 | 0.49 Â± 0.01 | 0.90 Â± 0.01    | 50.77 Â± 3.45 | 0.094 Â± 0.001 | 0.213 Â± 0.001 | 30.49 Â± 0.26 | 0.46 Â± 0.01 | 0.72 Â± 0.01 |
|                | AS       | 83.23 Â± 5.30 | 0.217 Â± 0.001 | 0.277 Â± 0.001 | 36.77 Â± 0.50 | 0.48 Â± 0.01 | 0.87 Â± 0.01    | 78.05 Â± 7.80 | 0.114 Â± 0.001 | 0.152 Â± 0.001 | 24.68 Â± 0.16 | 0.42 Â± 0.01 | 0.72 Â± 0.01 |
